{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acknowledged-african",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import arrow\n",
    "import pprint\n",
    "import json\n",
    "from urllib.parse import urlencode\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eleven-resistance",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = open(\"./NOTION_TOKEN\", \"r\").readlines()[0]\n",
    "notion_version =  \"2021-08-16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "pacific-portugal",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_data = {\"filter\": {\"and\": [{\"property\": \"标签\",\n",
    "                                  \"multi_select\": {\"is_not_empty\": True}},],},}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "responsible-teens",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_database = requests.post(\n",
    "    url=\"https://api.notion.com/v1/databases/cecf4bb039dc46bca130a29a9db58906/query\",\n",
    "    headers={\"Authorization\": \"Bearer \" + token,\n",
    "             \"Notion-Version\": notion_version,\n",
    "             \"Content-Type\": \"application/json\",\n",
    "             },\n",
    "    data=json.dumps(extra_data),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "baking-verse",
   "metadata": {},
   "outputs": [],
   "source": [
    "respond = json.loads(r_database.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bound-adams",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_page_plain_text(respond: dict):\n",
    "    for result in respond[\"results\"]:\n",
    "        page_id = result[\"url\"].split(\"/\")[-1].split(\"-\")[-1]\n",
    "        r_page = requests.get(\n",
    "                    url=f\"https://api.notion.com/v1/blocks/{page_id}/children\",\n",
    "                    headers={\"Authorization\": f\"Bearer {token}\",\n",
    "                             \"Notion-Version\": notion_version,\n",
    "                             \"Content-Type\": \"application/json\",\n",
    "                             },\n",
    "                    )\n",
    "        for block in json.loads(r_page.text).get(\"results\", []):\n",
    "            for key in block:\n",
    "                if not isinstance(block[key], dict):\n",
    "                    continue\n",
    "                if \"text\" not in block[key]:\n",
    "                    continue\n",
    "                for text in block[key][\"text\"]:\n",
    "                    yield text[\"plain_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "concerned-intent",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = list(take_page_plain_text(respond))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9304ef77-63f3-4703-bcd1-990f60843b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['别听他说什么，要看他做什么：关于总理和掏粪工只是分工不同的反驳，医院里只有高干病房，掏粪工进不去', '要点', '加了爱尔兰']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "considerable-silicon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "valued-death",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from unicodedata import category\n",
    "codepoints = range(sys.maxunicode + 1)\n",
    "punctuation = {c for k in codepoints if category(c := chr(k)).startswith(\"P\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "palestinian-rwanda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.491 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "from functional import seq\n",
    "split_text_list = [jieba.lcut(text, HMM=True) for text in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "assisted-priority",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "stopfiles = glob(\"./stopwords/*stopwords.txt\")\n",
    "\n",
    "stopwords = reduce(lambda x,y: x.union(y), [set([x.strip() for x in open(file, \"r\").readlines()]) for file in stopfiles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "formed-poker",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_stopwords(word):\n",
    "    return word in stopwords \\\n",
    "        or word in punctuation \\\n",
    "        or word.isdigit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "decent-rochester",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = seq(split_text_list).map(lambda sent: [word for word in sent if not check_stopwords(word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "sonic-vintage",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueWords = (sequence\n",
    "               .map(lambda sent: set(sent))\n",
    "               .reduce(lambda x, y: x.union(y))\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "stuck-tactics",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2sents = {word.lower(): set() for word in uniqueWords}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "voluntary-accuracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in text_list:\n",
    "    for word in uniqueWords:\n",
    "        if word in text:\n",
    "            word2sents[word.lower()].add(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-suite",
   "metadata": {},
   "source": [
    "## 现有库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "broke-street",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zdc/.cache/pypoetry/virtualenvs/jupyter-env-st7wAHic-py3.8/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(sequence.map(lambda x: \" \".join(x)).to_list())\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "accepted-frontier",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.max(axis=0).sort_values(key=lambda x: -x).to_csv(\"./tf_idf_topic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "pressed-nicholas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0x320646e7b37d5a31f5dcef9ccff9180eeb63b004\n",
      "{'0x320646e7b37d5a31f5dcef9ccff9180eeb63b004'}\n",
      "----------\n",
      "补充\n",
      "{'CV补充', '补充'}\n",
      "----------\n",
      "分散\n",
      "{'分散>集中', '注意力分散（大脑很难同时专注于2件事，其中一件事会倾向欲望和直觉）'}\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for word in df.max(axis=0).sort_values(key=lambda x: -x).head(3).index:\n",
    "    print(word)\n",
    "    print(word2sents[word])\n",
    "    print(\"-\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-terrain",
   "metadata": {},
   "source": [
    "## 自定义(不是tf*idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "downtown-dimension",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueWords = (sequence\n",
    "               .map(lambda sent: set(sent))\n",
    "               .reduce(lambda x, y: x.union(y))\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "prostate-alberta",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, bagOfWords):\n",
    "    tfDict = {}\n",
    "    bagOfWordsCount = len(bagOfWords)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(bagOfWordsCount)\n",
    "    return tfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "significant-inspection",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(documents):\n",
    "    import math\n",
    "    N = len(documents)\n",
    "    \n",
    "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
    "    for document in documents:\n",
    "        for word, val in document.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log(N / float(val))\n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfied-precipitation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
