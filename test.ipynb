{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acknowledged-african",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import arrow\n",
    "import pprint\n",
    "import json\n",
    "from urllib.parse import urlencode\n",
    "from functools import reduce\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "955705ac-fe04-41aa-b423-4348f1a83ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from utils.log import config_log\n",
    "\n",
    "config_log(\n",
    "        \"notion_api\",\n",
    "        \"DBtexts\",\n",
    "        log_root='./logs',\n",
    "        print_terminal=True,\n",
    "        enable_monitor=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cba7a0-735a-4654-afae-f45d6ae30ec0",
   "metadata": {},
   "source": [
    "## 环境变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c1b96ef-658f-4265-bc71-190b29e6bbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取token：https://www.notion.so/my-integrations/\n",
    "token = open(\"./NOTION_TOKEN\", \"r\").readlines()[0]\n",
    "# notion_version =  \"2021-08-16\"\n",
    "notion_version = \"2022-06-28\"\n",
    "\n",
    "notion_header = {\"Authorization\": f\"Bearer {token}\",\n",
    "                 \"Notion-Version\": notion_version,\n",
    "                 \"Content-Type\": \"application/json\",\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c7707f0-e794-4333-8663-d55b77762b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需要读取的database ID\n",
    "database_id = 'a2594f51053a47b3a58a171017ea0435'\n",
    "\n",
    "# 筛选 property，这里的 Label 是上述 database 中的属性\n",
    "extra_data = {\"filter\": {\"and\": [{\"property\": \"Label\",\n",
    "                                  \"multi_select\": {\"is_not_empty\": True}\n",
    "                                 },\n",
    "                                 {\"property\": \"Label\",\n",
    "                                  \"multi_select\": {\"contains\": \"思考\"}\n",
    "                                 }\n",
    "                                ],\n",
    "                        },\n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fb3887-99e6-4621-9593-d51117f59312",
   "metadata": {},
   "source": [
    "## 读取数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "293a1282-acd1-4410-8bfa-17c678bce36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "class NotionDBText:\n",
    "    \"\"\"\n",
    "    读取数据库中所有富文本信息\n",
    "    \"\"\"\n",
    "    def __init__(self, notion_header: dict, database_id: str, extra_data: dict = dict()):\n",
    "        self.header = notion_header\n",
    "        self.database_id = database_id\n",
    "        self.extra_data = extra_data\n",
    "        self.total_texts, self.total_blocks, self.total_pages = [[]] * 3\n",
    "        self.block_types = [\"paragraph\", \"bulleted_list_item\", \"numbered_list_item\", \n",
    "                            \"toggle\", \"to_do\", \"quote\", \n",
    "                            \"callout\", \"synced_block\", \"template\", \n",
    "                            \"column\", \"child_page\", \"child_database\", \"table\",\n",
    "                            \"heading_1\",\"heading_2\",\"heading_3\"]\n",
    "    \n",
    "    def read(self):\n",
    "        self.total_pages = self.read_pages()\n",
    "        self.total_blocks = self.read_blocks(self.total_pages)\n",
    "        self.total_texts = self.read_rich_text(self.total_blocks)\n",
    "        \n",
    "    def read_pages(self):\n",
    "        \"\"\"\n",
    "        读取database中所有pages\n",
    "        \"\"\"\n",
    "        total_pages = []\n",
    "        has_more = True\n",
    "        next_cursor = ''\n",
    "        # 有下一页时，继续读取\n",
    "        while has_more:\n",
    "            if next_cursor:\n",
    "                extra_data['start_cursor'] = next_cursor\n",
    "            r_database = requests.post(\n",
    "                url=f\"https://api.notion.com/v1/databases/{self.database_id}/query\",\n",
    "                headers=self.header,\n",
    "                data=json.dumps(self.extra_data),\n",
    "            )\n",
    "            respond = json.loads(r_database.text)\n",
    "            total_pages.extend(respond[\"results\"])\n",
    "            has_more = respond['has_more']\n",
    "            next_cursor = respond['next_cursor']\n",
    "        logging.info(f'{len(total_pages)} pages when {arrow.now()}')\n",
    "        return total_pages\n",
    "    \n",
    "    def read_blocks(self, pages: List):\n",
    "        \"\"\"\n",
    "        读取pages中所有blocks\n",
    "        \"\"\"\n",
    "        total_blocks = []\n",
    "        for page in tqdm(pages, desc='read blocks'):\n",
    "            page_id = page[\"id\"]\n",
    "            r_page = requests.get(\n",
    "                        url=f\"https://api.notion.com/v1/blocks/{page_id}/children\",\n",
    "                        headers=self.header,\n",
    "                        )\n",
    "            total_blocks.append(json.loads(r_page.text).get(\"results\", []))\n",
    "        return total_blocks\n",
    "        \n",
    "    def read_rich_text(self, blocks: List):\n",
    "        \"\"\"\n",
    "        读取blocks中所有rich text\n",
    "        \"\"\"\n",
    "        total_texts = []\n",
    "        for page_blocks in blocks:\n",
    "            page_texts = []\n",
    "            for block in page_blocks:\n",
    "                if block['type'] not in self.block_types:\n",
    "                    logging.warning(block['type'] + ' not in type list')\n",
    "                    continue\n",
    "                try:\n",
    "                    page_texts.extend([x['plain_text'] for x in block[block['type']]['rich_text']])\n",
    "                except Exception as e:\n",
    "                    logging.error(block['type'] + '|' + json.dumps(block[block['type']]))\n",
    "            total_texts.append(page_texts)\n",
    "        return total_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05fa63bf-b675-438d-8335-705ac63ab580",
   "metadata": {},
   "outputs": [],
   "source": [
    "notion_db = NotionDBText(notion_header, database_id, extra_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d34f240-a760-414d-ba5c-b38edd9ecc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-29 15:36:48.323] [INFO] [339943] [410687785.py] [42] [49 pages when 2023-01-29T15:36:48.323805+00:00]\n",
      "read blocks: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:14<00:00,  3.29it/s]\n",
      "[2023-01-29 15:37:03.246] [ERROR] [339943] [410687785.py] [73] [synced_block|{\"synced_from\": null}]\n",
      "[2023-01-29 15:37:03.247] [WARNING] [339943] [410687785.py] [68] [divider not in type list]\n",
      "[2023-01-29 15:37:03.250] [WARNING] [339943] [410687785.py] [68] [divider not in type list]\n"
     ]
    }
   ],
   "source": [
    "notion_db.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27820be7-444b-4811-aed4-50e87cbd1e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['不对，要削减的是中产，奢侈品与底层关系不大',\n",
       " '关于社会核心本质的思考',\n",
       " '控制、稳定',\n",
       " '奢侈品是为了小成本消耗生产力，金钱，让底层资产消耗掉，继续劳作，保持社会结构稳定']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notion_db.total_texts[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16d3e1f-2d37-46f4-979e-644fa11b4549",
   "metadata": {},
   "source": [
    "## 分析结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cba1e404-4489-4fc9-a7ab-af557e8f03ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U pkuseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9e54db5-4db6-4478-98ba-5241e694ac69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://github.com/lancopku/pkuseg-python/releases/download/v0.0.25/default_v2.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dc1160a-c734-4559-83cd-6149ce2d7468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pkuseg\n",
    "# from pathlib import Path\n",
    "\n",
    "# seg = pkuseg.pkuseg(model_name=Path('./pkuseg_model'))  # 程序会自动下载所对应的细领域模型\n",
    "\n",
    "# text = seg.cut('我爱北京天安门')              # 进行分词\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83f7b80f-e62c-4f9f-af94-0d80b353bcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn jieba -q\n",
    "\n",
    "import pandas as pd\n",
    "import jieba\n",
    "\n",
    "# 标点符号\n",
    "import sys\n",
    "from unicodedata import category\n",
    "codepoints = range(sys.maxunicode + 1)\n",
    "punctuation = {c for k in codepoints if category(c := chr(k)).startswith(\"P\")}\n",
    "\n",
    "# 停用词\n",
    "from glob import glob\n",
    "stopfiles = glob(\"./stopwords/*stopwords.txt\")\n",
    "stopwords = reduce(lambda x,y: x.union(y), [set([x.strip() for x in open(file, \"r\").readlines()]) for file in stopfiles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "formed-poker",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_stopwords(word: str):\n",
    "    \"\"\"\n",
    "    检查词语是否在停用词列表内\n",
    "    \"\"\"\n",
    "    return word in stopwords \\\n",
    "             or word.isdigit() \\\n",
    "             or word in punctuation \\\n",
    "             or not word.strip()\n",
    "\n",
    "def check_sentence_available(text: str):\n",
    "    \"\"\"\n",
    "    检查句子是否符合要求\n",
    "    \"\"\"\n",
    "    # 不要#开头的，可能是作为标签输入的\n",
    "    if text.startswith(\"#\"):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3bacd1-196c-4fcd-8434-a856087c2667",
   "metadata": {},
   "source": [
    "### 分词、清洗、建立映射"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c0a3e24-bed7-4ac0-b30c-b70a34750a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "[2023-01-29 15:37:04.181] [DEBUG] [339943] [__init__.py] [113] [Building prefix dict from the default dictionary ...]\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "[2023-01-29 15:37:04.190] [DEBUG] [339943] [__init__.py] [132] [Loading model from cache /tmp/jieba.cache]\n",
      "Loading model cost 0.825 seconds.\n",
      "[2023-01-29 15:37:05.015] [DEBUG] [339943] [__init__.py] [164] [Loading model cost 0.825 seconds.]\n",
      "Prefix dict has been built successfully.\n",
      "[2023-01-29 15:37:05.022] [DEBUG] [339943] [__init__.py] [166] [Prefix dict has been built successfully.]\n"
     ]
    }
   ],
   "source": [
    "from functional import seq\n",
    "text_list = [text for item in notion_db.total_texts for text in item  if check_sentence_available(text)]\n",
    "# 分词\n",
    "split_text_list = [jieba.lcut(text, HMM=True) for text in text_list]\n",
    "# 剔除停用词\n",
    "sequence = seq(split_text_list).map(lambda sent: [word for word in sent if not check_stopwords(word)])\n",
    "# sequence = seq(split_text_list)\n",
    "\n",
    "# 包含的词\n",
    "uniqueWords = (sequence\n",
    "               .map(lambda sent: set(sent))\n",
    "               .reduce(lambda x, y: x.union(y))\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fd26cfe-f7f6-401c-920a-73bfc4cd9090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词 --> 句子 查询字典\n",
    "word2sents = {word.lower(): set() for word in uniqueWords}\n",
    "\n",
    "for text in text_list:\n",
    "    for word in uniqueWords:\n",
    "        if word in text:\n",
    "            word2sents[word.lower()].add(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-international",
   "metadata": {},
   "source": [
    "### 使用标准tf-idf工具来分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "broke-street",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(sequence.map(lambda x: \" \".join(x)).to_list())\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "denselist = vectors.todense().tolist()\n",
    "df = pd.DataFrame(denselist, columns=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc67d154-63a7-436c-9f2e-84d7673fc1ec",
   "metadata": {},
   "source": [
    "#### 按不同统计方法逆序输出所有词的tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b5fb000-fb95-4ce7-9da1-60cbc64b62bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# 存储目录\n",
    "directory = Path('./results')\n",
    "directory.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c7b850b-e922-48aa-baa1-e7e9c8eae465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 剔除最大最小值，求均值\n",
    "df_drop_maxmin = df.copy()\n",
    "for col in df.columns:\n",
    "    df_drop_maxmin[col] = df[col][df[col].between(df[col].min(), df[col].max())]\n",
    "    df_drop_maxmin[col].dropna(inplace=True)\n",
    "df_drop_maxmin.mean().sort_values(ascending=False).to_csv(directory / \"tf_idf_top.drop_maxmin.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "accepted-frontier",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最大值\n",
    "df.max(axis=0).sort_values(ascending=False).to_csv(directory / \"tf_idf_top.max.csv\")\n",
    "# 求和\n",
    "df.sum(axis=0).sort_values(ascending=False).to_csv(directory / \"tf_idf_top.sum.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "pressed-nicholas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查高频词\n",
    "with open(directory / \"top_word_with_sents.md\", \"w\") as f:\n",
    "    for word in df.sum(axis=0).sort_values(ascending=False).head(20).index:\n",
    "        f.write('### '+ word + '\\n\\n')\n",
    "        f.write('\\n\\n'.join([sent.replace(\"\\n\", \" \").replace(word, f'**{word}**') for sent in word2sents[word]]) + '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-violation",
   "metadata": {},
   "source": [
    "### 自定义(不是tf*idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fitting-hebrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, bagOfWords):\n",
    "    tfDict = {}\n",
    "    bagOfWordsCount = len(bagOfWords)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(bagOfWordsCount)\n",
    "    return tfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "analyzed-setting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(documents):\n",
    "    import math\n",
    "    N = len(documents)\n",
    "    \n",
    "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
    "    for document in documents:\n",
    "        for word, val in document.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log(N / float(val))\n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75bd4cc-5aac-4b60-b227-d85b5b568f26",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033b3a4a-6b52-4cc0-971e-17900f6270db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
