{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "important-miniature",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import arrow\n",
    "import pprint\n",
    "import json\n",
    "from urllib.parse import urlencode\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "italian-synthesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = open(\"./NOTION_TOKEN\", \"r\").readlines()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "juvenile-prague",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_data = {\"filter\": {\"and\": [{\"property\": \"标签\",\n",
    "                                  \"multi_select\": {\"is_not_empty\": True}},],},}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "proof-remains",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_database = requests.post(\n",
    "    url=\"https://api.notion.com/v1/databases/cecf4bb039dc46bca130a29a9db58906/query\",\n",
    "    headers={\"Authorization\": \"Bearer \" + token,\n",
    "             \"Notion-Version\": \"2021-08-02\",\n",
    "             \"Content-Type\": \"application/json\",\n",
    "             },\n",
    "    data=json.dumps(extra_data),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "mobile-revolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "respond = json.loads(r_database.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "missing-month",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_page_plain_text(respond: dict):\n",
    "    for result in respond[\"results\"]:\n",
    "        page_id = result[\"url\"].split(\"/\")[-1].split(\"-\")[-1]\n",
    "        r_page = requests.get(\n",
    "                    url=f\"https://api.notion.com/v1/blocks/{page_id}/children\",\n",
    "                    headers={\"Authorization\": f\"Bearer {token}\",\n",
    "                             \"Notion-Version\": \"2021-08-02\",\n",
    "                             \"Content-Type\": \"application/json\",\n",
    "                             },\n",
    "                    )\n",
    "        for block in json.loads(r_page.text).get(\"results\", []):\n",
    "            for key in block:\n",
    "                if not isinstance(block[key], dict):\n",
    "                    continue\n",
    "                if \"text\" not in block[key]:\n",
    "                    continue\n",
    "                for text in block[key][\"text\"]:\n",
    "                    yield text[\"plain_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "numerous-contract",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'object': 'list', 'results': [], 'next_cursor': None, 'has_more': False}\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "text_list = list(take_page_plain_text(respond))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "institutional-glass",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "organic-country",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import sys'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unicodedata import category\n",
    "codepoints = range(sys.maxunicode + 1)\n",
    "punctuation = {c for k in codepoints if category(c := chr(k)).startswith(\"P\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "global-serial",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functional import seq\n",
    "split_text_list = [jieba.lcut(text, HMM=True) for text in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "invalid-samoa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "stopfiles = glob(\"./stopwords/*stopwords.txt\")\n",
    "\n",
    "stopwords = reduce(lambda x,y: x.union(y), [set([x.strip() for x in open(file, \"r\").readlines()]) for file in stopfiles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "acoustic-alert",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_stopwords(word):\n",
    "    return word in stopwords \\\n",
    "        or word in punctuation \\\n",
    "        or word.isdigit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "innovative-humidity",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = seq(split_text_list).map(lambda sent: [word for word in sent if not check_stopwords(word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "lucky-switch",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueWords = (sequence\n",
    "               .map(lambda sent: set(sent))\n",
    "               .reduce(lambda x, y: x.union(y))\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "integral-ribbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2sents = {word.lower(): set() for word in uniqueWords}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "confident-noise",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in text_list:\n",
    "    for word in uniqueWords:\n",
    "        if word in text:\n",
    "            word2sents[word.lower()].add(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-international",
   "metadata": {},
   "source": [
    "## 现有库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "confidential-shanghai",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(sequence.map(lambda x: \" \".join(x)).to_list())\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "awful-inflation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.max(axis=0).sort_values(key=lambda x: -x).to_csv(\"./tf_idf_topic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "changed-arthritis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "枯燥\n",
      "{'），较为枯燥'}\n",
      "----------\n",
      "nobility\n",
      "{'nobility', '可以批注二度思考nobility的手写笔记，组织成结构'}\n",
      "----------\n",
      "xmind\n",
      "{'xmind层级清晰，梳理逻辑', 'xmind'}\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for word in df.max(axis=0).sort_values(key=lambda x: -x).head(3).index:\n",
    "    print(word)\n",
    "    print(word2sents[word])\n",
    "    print(\"-\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-violation",
   "metadata": {},
   "source": [
    "## 自定义(不是tf*idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cheap-sense",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueWords = (sequence\n",
    "               .map(lambda sent: set(sent))\n",
    "               .reduce(lambda x, y: x.union(y))\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fitting-hebrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, bagOfWords):\n",
    "    tfDict = {}\n",
    "    bagOfWordsCount = len(bagOfWords)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(bagOfWordsCount)\n",
    "    return tfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "analyzed-setting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(documents):\n",
    "    import math\n",
    "    N = len(documents)\n",
    "    \n",
    "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
    "    for document in documents:\n",
    "        for word, val in document.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log(N / float(val))\n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-disease",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
