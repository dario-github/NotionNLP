{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1ea8ec-98b6-4148-ba91-cfb2d8101a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import arrow\n",
    "    from tqdm import tqdm\n",
    "    import pandas as pd\n",
    "    from functional import seq\n",
    "    from ruamel.yaml import YAML\n",
    "    import jieba\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "except ModuleNotFoundError:\n",
    "    %pip install arrow ruamel.yaml tqdm pandas pyfunctional scikit-learn jieba -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-african",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from urllib.parse import urlencode\n",
    "from pathlib import Path\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955705ac-fe04-41aa-b423-4348f1a83ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 日志模块\n",
    "import logging\n",
    "from notion_rich_text_analysis.parameter.log import config_log\n",
    "\n",
    "config_log(\n",
    "        \"notion_rich_text_analysis\",\n",
    "        \"notebook\",\n",
    "        log_root='notion_rich_text_analysis/logs',\n",
    "        print_terminal=True,\n",
    "        enable_monitor=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd80c8e-39c1-43a1-aa5f-1d03aca239a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 yaml 1.2\n",
    "from ruamel.yaml import YAML\n",
    "yaml = YAML()\n",
    "## define custom tag handler\n",
    "def join(loader, node):\n",
    "    seq = loader.construct_sequence(node)\n",
    "    return ''.join([str(i) for i in seq])\n",
    "\n",
    "## register the tag handler\n",
    "yaml.constructor.add_constructor('!join', join)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44cba7a0-735a-4654-afae-f45d6ae30ec0",
   "metadata": {},
   "source": [
    "## 变量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaad5ea2-ae82-425f-96b6-529761f75650",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml', 'r', encoding='utf-8') as f:\n",
    "    config = yaml.load(f)\n",
    "\n",
    "# request的header信息\n",
    "notion_header = config['notion']['header']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63fb3887-99e6-4621-9593-d51117f59312",
   "metadata": {},
   "source": [
    "## 读取数据库\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293a1282-acd1-4410-8bfa-17c678bce36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "class NotionDBText:\n",
    "    \"\"\"\n",
    "    读取数据库中所有富文本信息\n",
    "    \"\"\"\n",
    "    def __init__(self, header: dict, database_id: str, extra_data: dict = dict()):\n",
    "        self.header = header\n",
    "        self.database_id = database_id\n",
    "        self.extra_data = extra_data\n",
    "        self.total_texts, self.total_blocks, self.total_pages = [[]] * 3\n",
    "        self.block_types = [\"paragraph\", \"bulleted_list_item\", \"numbered_list_item\", \n",
    "                            \"toggle\", \"to_do\", \"quote\", \n",
    "                            \"callout\", \"synced_block\", \"template\", \n",
    "                            \"column\", \"child_page\", \"child_database\", \"table\",\n",
    "                            \"heading_1\",\"heading_2\",\"heading_3\"]\n",
    "    \n",
    "    def read(self):\n",
    "        self.total_pages = self.read_pages()\n",
    "        self.total_blocks = self.read_blocks(self.total_pages)\n",
    "        self.total_texts = self.read_rich_text(self.total_blocks)\n",
    "        \n",
    "    def read_pages(self):\n",
    "        \"\"\"\n",
    "        读取database中所有pages\n",
    "        \"\"\"\n",
    "        total_pages = []\n",
    "        has_more = True\n",
    "        next_cursor = ''\n",
    "        # 有下一页时，继续读取\n",
    "        while has_more:\n",
    "            if next_cursor:\n",
    "                self.extra_data['start_cursor'] = next_cursor\n",
    "            r_database = requests.post(\n",
    "                url=f\"https://api.notion.com/v1/databases/{self.database_id}/query\",\n",
    "                headers=self.header,\n",
    "                data=json.dumps(self.extra_data),\n",
    "            )\n",
    "            respond = json.loads(r_database.text)\n",
    "            total_pages.extend(respond[\"results\"])\n",
    "            has_more = respond['has_more']\n",
    "            next_cursor = respond['next_cursor']\n",
    "        logging.info(f'{len(total_pages)} pages when {arrow.now()}')\n",
    "        return total_pages\n",
    "    \n",
    "    def read_blocks(self, pages: List):\n",
    "        \"\"\"\n",
    "        读取pages中所有blocks\n",
    "        \"\"\"\n",
    "        total_blocks = []\n",
    "        for page in tqdm(pages, desc='read blocks'):\n",
    "            page_id = page[\"id\"]\n",
    "            r_page = requests.get(\n",
    "                        url=f\"https://api.notion.com/v1/blocks/{page_id}/children\",\n",
    "                        headers=self.header,\n",
    "                        )\n",
    "            total_blocks.append(json.loads(r_page.text).get(\"results\", []))\n",
    "        return total_blocks\n",
    "        \n",
    "    def read_rich_text(self, blocks: List):\n",
    "        \"\"\"\n",
    "        读取blocks中所有rich text\n",
    "        \"\"\"\n",
    "        total_texts = []\n",
    "        self.unsupported_types = set()\n",
    "        for page_blocks in blocks:\n",
    "            page_texts = []\n",
    "            for block in page_blocks:\n",
    "                if block['type'] not in self.block_types:\n",
    "#                     logging.warning(block['type'] + ' not in type list')\n",
    "                    self.unsupported_types.add(block['type'])\n",
    "                    continue\n",
    "                try:\n",
    "                    page_texts.extend([x['plain_text'] for x in block[block['type']]['rich_text']])\n",
    "                except Exception as e:\n",
    "                    logging.error(block['type'] + '|' + json.dumps(block[block['type']]))\n",
    "            total_texts.append(page_texts)\n",
    "        return total_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9e0358-8c82-43c5-8a91-92aeac5fef92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notion_db = NotionDBText()\n",
    "\n",
    "# notion_db.read()\n",
    "\n",
    "# notion_db.total_texts[-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f16d3e1f-2d37-46f4-979e-644fa11b4549",
   "metadata": {},
   "source": [
    "## 分析结果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae6c12c-adfe-4dcf-a26a-14bfc658c90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U pkuseg\n",
    "\n",
    "# !wget https://github.com/lancopku/pkuseg-python/releases/download/v0.0.25/default_v2.zip\n",
    "\n",
    "# import pkuseg\n",
    "# from pathlib import Path\n",
    "\n",
    "# seg = pkuseg.pkuseg(model_name=Path('./pkuseg_model'))  # 程序会自动下载所对应的细领域模型\n",
    "\n",
    "# text = seg.cut('我爱北京天安门')              # 进行分词\n",
    "# print(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69727794-348f-4337-b8ad-2f924af2cbc8",
   "metadata": {},
   "source": [
    "### 停用词\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f7b80f-e62c-4f9f-af94-0d80b353bcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn jieba -q\n",
    "\n",
    "# 标点符号\n",
    "import sys\n",
    "from unicodedata import category\n",
    "codepoints = range(sys.maxunicode + 1)\n",
    "punctuation = {c for k in codepoints if category(c := chr(k)).startswith(\"P\")}\n",
    "\n",
    "# 停用词\n",
    "from glob import glob\n",
    "stopfiles = glob(\"./stopwords/*stopwords.txt\")\n",
    "stopwords = reduce(lambda x,y: x.union(y), \n",
    "                   [set([x.strip() \n",
    "                         for x in open(file, \"r\").readlines()])\n",
    "                         for file in stopfiles])\n",
    "stopwords = stopwords | punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe837877-ddbf-4c97-a52c-052ea1ceadff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotionTextAnalysis(NotionDBText):\n",
    "    '''\n",
    "    分析notion富文本信息\n",
    "    '''\n",
    "    def __init__(self, header, task_name, task_describe, database_id, extra_data):\n",
    "        super().__init__(header, database_id, extra_data)\n",
    "        logging.info(f'{task_name} start, {task_describe}')\n",
    "        self.read()\n",
    "        logging.info(f'Unsupported types: {self.unsupported_types}')\n",
    "        \n",
    "        self.task_name = task_name\n",
    "        self.task_describe = task_describe\n",
    "        self.database_id = database_id\n",
    "        self.extra_data = extra_data\n",
    "    \n",
    "    def run(self, stopwords=set(), output_dir=Path('./'), top_n=5):\n",
    "        self.handling_sentences(stopwords)\n",
    "        self.tf_idf_dataframe = self.tf_idf(self.sequence)\n",
    "        self.output(self.task_name, self.task_describe, output_dir, top_n)\n",
    "        \n",
    "    @staticmethod\n",
    "    def check_stopwords(word: str, stopwords: set):\n",
    "        \"\"\"\n",
    "        检查词语是否在停用词列表内\n",
    "        \"\"\"\n",
    "        return word in stopwords \\\n",
    "                 or word.isdigit() \\\n",
    "                 or not word.strip()\n",
    "    \n",
    "    @staticmethod\n",
    "    def check_sentence_available(text: str):\n",
    "        \"\"\"\n",
    "        检查句子是否符合要求\n",
    "        \"\"\"\n",
    "        # 不要#开头的，可能是作为标签输入的，也可以用来控制一些分版本的重复内容\n",
    "        if text.startswith(\"#\"):\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def handling_sentences(self, stopwords):\n",
    "        '''\n",
    "        处理所有文本：分词、清洗、建立映射\n",
    "        '''\n",
    "        import jieba\n",
    "        \n",
    "        # 检查数据库中获取的富文本是否为空\n",
    "        if not self.total_texts:\n",
    "            logging.error(f'该任务未获取到符合条件的文本，请检查筛选条件。database ID: {self.database_id}; extra data: {self.extra_data}')\n",
    "            raise ValueError('empty rich texts.')\n",
    "        \n",
    "        # 剔除无效句子\n",
    "        text_list = [text for item in self.total_texts \n",
    "                     for text in item  \n",
    "                     if self.check_sentence_available(text)]\n",
    "        # 分词\n",
    "        split_text_list = [jieba.lcut(text, HMM=True) for text in text_list]\n",
    "        \n",
    "        # 剔除停用词\n",
    "        self.sequence = seq(split_text_list).map(\n",
    "            lambda sent: [word for word in sent \n",
    "                          if not self.check_stopwords(word, stopwords)])\n",
    "\n",
    "        # 检查序列是否为空\n",
    "        if not self.sequence:\n",
    "            logging.error(f'该任务未获取到符合条件的文本，请检查停用词。database ID: {self.database_id}; extra data: {self.extra_data}')\n",
    "            raise ValueError('empty rich texts.')\n",
    "        \n",
    "        # 获取词表\n",
    "        self.unique_words = (self.sequence\n",
    "                           .map(lambda sent: set(sent))\n",
    "                           .reduce(lambda x, y: x.union(y)))\n",
    "        \n",
    "        # 检查词表是否为空\n",
    "        if not self.unique_words:\n",
    "            logging.error(f'词表为空，请检查筛选条件及停用词。database ID: {self.database_id}; extra data: {self.extra_data}')\n",
    "            raise ValueError('empty unique words')\n",
    "        \n",
    "        # 词 --> 句子 查询字典\n",
    "        self.word2sents = self._word2sent(text_list, self.unique_words)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _word2sent(text_list, unique_words):\n",
    "        '''\n",
    "        词 --> 句子 查询字典\n",
    "        '''\n",
    "        word2sents = {word.lower(): set() for word in unique_words}\n",
    "\n",
    "        for text in text_list:\n",
    "            for word in unique_words:\n",
    "                if word in text:\n",
    "                    word2sents[word.lower()].add(text)\n",
    "        return word2sents\n",
    "    \n",
    "    @staticmethod\n",
    "    def tf_idf(sequence):\n",
    "        '''\n",
    "        使用标准tf-idf工具来分析\n",
    "        '''\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        vectors = vectorizer.fit_transform(sequence.map(lambda x: \" \".join(x)).to_list())\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        denselist = vectors.todense().tolist()\n",
    "        df = pd.DataFrame(denselist, columns=feature_names)\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def empty_func(*args, **kwargs):\n",
    "        return\n",
    "\n",
    "    def output(self, task_name, task_describe, output_dir=Path('./'), top_n=5):\n",
    "        '''\n",
    "        输出分析结果\n",
    "        '''\n",
    "        import re\n",
    "        \n",
    "        self.directory = Path(output_dir)\n",
    "        self.directory.mkdir(exist_ok=True)\n",
    "        \n",
    "        # 按不同统计方法逆序输出所有词的tf-idf\n",
    "        result_type = 'tf_idf'\n",
    "        task_name_clean = re.sub(r\"\\s\", \"_\", task_name.strip())\n",
    "        result_suffix = f'{task_name_clean}.{result_type}.analysis_result'\n",
    "        result_attr_list = ['by_mean_drop_maxmin', 'by_max', 'by_sum']\n",
    "        for attr in result_attr_list:\n",
    "            func = getattr(self, attr, empty_func)(self.tf_idf_dataframe)\n",
    "            if not func:\n",
    "                continue\n",
    "            func.to_csv(self.directory / f\"{result_suffix}.{attr}.csv\")\n",
    "        self.top_freq(self.tf_idf_dataframe, \n",
    "                        f'{result_suffix}.top{top_n}_word_with_sentences.md', task_describe, top_n)\n",
    "        \n",
    "        logging.info(f'{self.task_name} result files have been saved to {output_dir}.')\n",
    "    \n",
    "    def top_freq(self, df, file_name, task_describe, top_n):\n",
    "        '''\n",
    "        检查高频词\n",
    "        '''\n",
    "        with open(self.directory / file_name, \"w\") as f:\n",
    "            f.write('# ' + task_describe + '\\n\\n')\n",
    "            for word in df.sum(axis=0).sort_values(ascending=False).head(top_n).index:\n",
    "                f.write('## '+ word + '\\n\\n')\n",
    "                f.write('\\n\\n'.join([sent.replace(\"\\n\", \" \").replace(word, f'**{word}**') for sent in self.word2sents[word]]) + '\\n\\n')\n",
    "\n",
    "    @staticmethod\n",
    "    def by_mean_drop_maxmin(df):\n",
    "        # 剔除最大最小值，求均值\n",
    "        df_drop_maxmin = df.copy()\n",
    "        for col in df.columns:\n",
    "            df_drop_maxmin[col] = df[col][df[col].between(df[col].min(), df[col].max())]\n",
    "            df_drop_maxmin[col].dropna(inplace=True)\n",
    "        return df_drop_maxmin.mean().sort_values(ascending=False)\n",
    "        \n",
    "    @staticmethod\n",
    "    def by_max(df):\n",
    "        # 最大值\n",
    "        return df.max(axis=0).sort_values(ascending=False)\n",
    "    \n",
    "    @staticmethod\n",
    "    def by_sum(df):\n",
    "        # 求和\n",
    "        return df.sum(axis=0).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d7d60b-f58e-435f-81ca-7060318c7f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in config['task']:\n",
    "    if not task['run']:\n",
    "        continue\n",
    "    # 任务名称及描述\n",
    "    task_name = task['name']\n",
    "    task_describe = task['describe']\n",
    "\n",
    "    # 需要读取的database ID\n",
    "    database_id = task['database_id']\n",
    "\n",
    "    # 筛选 property，这里的 Label 是上述 database 中的属性\n",
    "    extra_data = task['extra']\n",
    "    try:\n",
    "        notion_text_analysis = NotionTextAnalysis(notion_header, task_name, task_describe, database_id, extra_data)\n",
    "        notion_text_analysis.run(stopwords, output_dir=Path('./results'), top_n=10)\n",
    "    except Exception as e:\n",
    "        logging.error(f'{task_name} failed. \\n{e}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "sublime-violation",
   "metadata": {},
   "source": [
    "### 自定义分析方法(不是 tf\\*idf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-hebrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, bagOfWords):\n",
    "    tfDict = {}\n",
    "    bagOfWordsCount = len(bagOfWords)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(bagOfWordsCount)\n",
    "    return tfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-setting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(documents):\n",
    "    import math\n",
    "    N = len(documents)\n",
    "    \n",
    "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
    "    for document in documents:\n",
    "        for word, val in document.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log(N / float(val))\n",
    "    return idfDict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e75bd4cc-5aac-4b60-b227-d85b5b568f26",
   "metadata": {},
   "source": [
    "## test\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-env-JJ-tZR_a-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "3c837ed23ab228efc5dbe21540abdd2e3665fb952415435c2d374a97d25336ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
